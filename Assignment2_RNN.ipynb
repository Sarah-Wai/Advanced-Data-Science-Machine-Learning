{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNTOQPAqKXgfzWs3F/GnKu6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ca91902140584e6eb33eba7e50baf127":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8a112cb077c4833bdc12102d8ec57cc","IPY_MODEL_bde926e56b3448b9a0ab602536d1f8b2","IPY_MODEL_48dc3c34ceb145afb3f0d23e7cb84e3d"],"layout":"IPY_MODEL_76916aa510964d0c914e13d15dea2656"}},"d8a112cb077c4833bdc12102d8ec57cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b1844d91d294695af1ff6c851cc29a4","placeholder":"​","style":"IPY_MODEL_9119ef68472549008a3092a7f6f0d156","value":"Map: 100%"}},"bde926e56b3448b9a0ab602536d1f8b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e71d2c0077fd493d8f904a64c19b4308","max":872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b807ce3402434ab096fe0f9add88ced9","value":872}},"48dc3c34ceb145afb3f0d23e7cb84e3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99aed8732d5458e95760e71fee6dad5","placeholder":"​","style":"IPY_MODEL_7c643a76fbc442bd8dc17fb9cb50383e","value":" 872/872 [00:00&lt;00:00, 5665.39 examples/s]"}},"76916aa510964d0c914e13d15dea2656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b1844d91d294695af1ff6c851cc29a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9119ef68472549008a3092a7f6f0d156":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e71d2c0077fd493d8f904a64c19b4308":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b807ce3402434ab096fe0f9add88ced9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e99aed8732d5458e95760e71fee6dad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c643a76fbc442bd8dc17fb9cb50383e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import os, json, random, numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","import matplotlib.pyplot as plt\n","import csv"],"metadata":{"id":"nK4zz_5BkDzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42\n","random.seed(SEED); np.random.seed(SEED)\n","torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"jF_FGwj_kFtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_sst2_and_tokenizer(tokenizer_name=\"bert-base-uncased\", max_len=64):\n","    raw = load_dataset(\"glue\", \"sst2\")\n","    tok = AutoTokenizer.from_pretrained(tokenizer_name)\n","    if tok.pad_token is None:\n","        tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n","    pad_id = tok.pad_token_id\n","    vocab_size = tok.vocab_size\n","\n","    def tok_fn(batch):\n","        out = tok(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=None)\n","        attn = out[\"attention_mask\"]\n","        lengths = [int(sum(m)) for m in attn]\n","        out2 = {\"input_ids\": out[\"input_ids\"], \"attention_mask\": attn, \"length\": lengths}\n","        if \"label\" in batch: out2[\"label\"] = batch[\"label\"]\n","        return out2\n","\n","    tokenized = {}\n","    for split in raw.keys():\n","        tokenized[split] = raw[split].map(tok_fn, batched=True, remove_columns=raw[split].column_names)\n","        tokenized[split].set_format(type=\"torch\")\n","    return tokenized, tok, vocab_size, pad_id"],"metadata":{"id":"MErcUVcIkNXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SST2TorchDataset(Dataset):\n","    def __init__(self, ds, is_test=False):\n","        self.ds = ds; self.is_test = is_test\n","    def __len__(self): return len(self.ds)\n","    def __getitem__(self, idx):\n","        ex = self.ds[idx]\n","        x = ex[\"input_ids\"]\n","        length = ex[\"length\"]\n","        y = -1 if self.is_test else int(ex[\"label\"])\n","        return x, length, y"],"metadata":{"id":"Y5rryLQSkSUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    xs, lengths, ys = zip(*batch)\n","    xs = torch.stack(xs, dim=0)\n","    lengths = torch.tensor(lengths, dtype=torch.long)\n","    ys = torch.tensor(ys, dtype=torch.long)\n","    lengths, perm = lengths.sort(descending=True)\n","    xs = xs[perm]; ys = ys[perm]\n","    return xs, lengths, ys"],"metadata":{"id":"RWDjNgQMkUoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BasicRNNSentiment(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0.3, pad_idx=0, num_classes=2):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n","        self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","    def forward(self, x, lengths):\n","        emb = self.embedding(x)\n","        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=True)\n","        _, h_n = self.rnn(packed)\n","        out = self.dropout(h_n[-1])\n","        return self.fc(out)"],"metadata":{"id":"8EZhc1WCkX0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GRUSentiment(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0.3, pad_idx=0, num_classes=2):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n","        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","    def forward(self, x, lengths):\n","        emb = self.embedding(x)\n","        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=True)\n","        _, h_n = self.gru(packed)\n","        out = self.dropout(h_n[-1])\n","        return self.fc(out)"],"metadata":{"id":"lJWCu2MCkafz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTMSentiment(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0.3, pad_idx=0, num_classes=2):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","    def forward(self, x, lengths):\n","        emb = self.embedding(x)\n","        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=True)\n","        _, (h_n, _) = self.lstm(packed)\n","        out = self.dropout(h_n[-1])\n","        return self.fc(out)"],"metadata":{"id":"bO9CPQRQkcLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BiLSTMSentiment(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, bidirectional=True, dropout=0.3, pad_idx=0, num_classes=2):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n","    def forward(self, x, lengths):\n","        emb = self.embedding(x)\n","        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=True)\n","        _, (h_n, _) = self.lstm(packed)\n","        h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)\n","        out = self.dropout(h_cat)\n","        return self.fc(out)"],"metadata":{"id":"PH4U2OPDkd9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_eval_one_model(model, name, train_dl, dev_dl, device, epochs=20, lr=1e-3, grad_clip=1.0):\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    best_f1 = 0.0\n","    best_path = f\"{name}_best.pt\"\n","    history = {'epoch': [], 'train_loss': [], 'train_acc': [], 'dev_loss': [], 'dev_acc': [], 'dev_f1': []}\n","\n","    for ep in range(1, epochs + 1):\n","        model.train()\n","        tot_loss = 0\n","        preds_all, labels_all = [], []\n","        for x, lengths, y in train_dl:\n","            x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            logits = model(x, lengths)\n","            loss = criterion(logits, y)\n","            loss.backward()\n","            if grad_clip is not None:\n","                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","            optimizer.step()\n","\n","            tot_loss += loss.item() * x.size(0)\n","            preds_all.extend(logits.argmax(1).detach().cpu().tolist())\n","            labels_all.extend(y.detach().cpu().tolist())\n","\n","        train_loss = tot_loss / len(train_dl.dataset)\n","        train_acc = accuracy_score(labels_all, preds_all)\n","        _, _, train_f1, _ = precision_recall_fscore_support(labels_all, preds_all, average=\"binary\", zero_division=0)\n","\n","        model.eval()\n","        tot_loss = 0\n","        preds_all, labels_all = [], []\n","        with torch.no_grad():\n","            for x, lengths, y in dev_dl:\n","                x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n","                logits = model(x, lengths)\n","                loss = criterion(logits, y)\n","                tot_loss += loss.item() * x.size(0)\n","                preds_all.extend(logits.argmax(1).detach().cpu().tolist())\n","                labels_all.extend(y.detach().cpu().tolist())\n","\n","        dev_loss = tot_loss / len(dev_dl.dataset)\n","        dev_acc = accuracy_score(labels_all, preds_all)\n","        p, r, dev_f1, _ = precision_recall_fscore_support(labels_all, preds_all, average=\"binary\", zero_division=0)\n","        cm = confusion_matrix(labels_all, preds_all)\n","\n","        history['epoch'].append(ep)\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['dev_loss'].append(dev_loss)\n","        history['dev_acc'].append(dev_acc)\n","        history['dev_f1'].append(dev_f1)\n","\n","        if dev_f1 > best_f1:\n","            best_f1 = dev_f1\n","            torch.save(model.state_dict(), best_path)\n","\n","        print(f\"[{name}] [Epoch {ep}] Train loss={train_loss:.4f} acc={train_acc:.4f} | Dev loss={dev_loss:.4f} acc={dev_acc:.4f} f1={dev_f1:.4f}\")\n","\n","    model.load_state_dict(torch.load(best_path, map_location=device))\n","    model.eval()\n","    preds_all, labels_all = [], []\n","    tot_loss = 0\n","    with torch.no_grad():\n","        for x, lengths, y in dev_dl:\n","            x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n","            logits = model(x, lengths)\n","            loss = criterion(logits, y)\n","            tot_loss += loss.item() * x.size(0)\n","            preds_all.extend(logits.argmax(1).detach().cpu().tolist())\n","            labels_all.extend(y.detach().cpu().tolist())\n","    final_loss = tot_loss / len(dev_dl.dataset)\n","    final_acc = accuracy_score(labels_all, preds_all)\n","    p, r, f1, _ = precision_recall_fscore_support(labels_all, preds_all, average=\"binary\", zero_division=0)\n","    final_cm = confusion_matrix(labels_all, preds_all)\n","\n","    summary = {\"dev_loss\": final_loss, \"dev_acc\": final_acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"cm\": final_cm.tolist()}\n","    return history, summary, best_path"],"metadata":{"id":"emPn57Raki8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------- Plotting ---------------------\n","def plot_history(history_dicts, out_dir=\"figs\"):\n","    \"\"\"\n","    history_dicts: dict(name -> history dict)\n","    Creates:\n","      - fig1: accuracy (all models on one chart)\n","      - fig2: loss (all models on one chart)\n","    \"\"\"\n","    os.makedirs(out_dir, exist_ok=True)\n","    # Figure 1: Accuracy\n","    plt.figure(figsize=(9,6))\n","    for name, h in history_dicts.items():\n","        plt.plot(h['epoch'], h['train_acc'], label=f\"{name} Train\")\n","        plt.plot(h['epoch'], h['dev_acc'],   label=f\"{name} Val\")\n","    plt.title(\"Figure 1: Training vs Validation Accuracy (All Models)\")\n","    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True)\n","    f1_path = os.path.join(out_dir, \"fig1_accuracy_all_models.png\")\n","    plt.savefig(f1_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n","\n","    # Figure 2: Loss\n","    plt.figure(figsize=(9,6))\n","    for name, h in history_dicts.items():\n","        plt.plot(h['epoch'], h['train_loss'], label=f\"{name} Train\")\n","        plt.plot(h['epoch'], h['dev_loss'],   label=f\"{name} Val\")\n","    plt.title(\"Figure 2: Training vs Validation Loss (All Models)\")\n","    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True)\n","    f2_path = os.path.join(out_dir, \"fig2_loss_all_models.png\")\n","    plt.savefig(f2_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n","    return f1_path, f2_path"],"metadata":{"id":"yM89hSgXklgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_f1_summary(summaries, out_dir=\"figs\"):\n","    os.makedirs(out_dir, exist_ok=True)\n","    names = list(summaries.keys())\n","    f1s = [summaries[n][\"f1\"] for n in names]\n","    plt.figure()\n","    plt.bar(names, f1s)\n","    for i, v in enumerate(f1s):\n","        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n","    plt.ylim(0, max(f1s) + 0.1)\n","    plt.title(\"Best Validation F1 by Architecture\")\n","    plt.ylabel(\"F1-score\"); plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n","    out_path = os.path.join(out_dir, \"summary_f1.png\")\n","    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n","    return out_path"],"metadata":{"id":"buUBcd5okpgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_confusion_matrix(cm, name, out_dir=\"figs\"):\n","    os.makedirs(out_dir, exist_ok=True)\n","    fig, ax = plt.subplots(figsize=(4.8,4))\n","    im = ax.imshow(cm)\n","    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n","    ax.set_xticklabels(['Negative','Positive']); ax.set_yticklabels(['Negative','Positive'])\n","    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n","    ax.set_title(f\"Figure 3: Confusion Matrix — {name}\")\n","    for i in range(2):\n","        for j in range(2):\n","            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n","    fig.tight_layout()\n","    out = os.path.join(out_dir, f\"fig3_cm_{name.lower()}.png\")\n","    plt.savefig(out, dpi=150, bbox_inches=\"tight\")\n","    plt.close()\n","    return out"],"metadata":{"id":"iNyQ3dyvZ9FX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------- Main ---------------------\n","def main():\n","    MAX_LEN = 64\n","    tokenized, tokenizer, vocab_size, pad_id = load_sst2_and_tokenizer(max_len=MAX_LEN)\n","\n","    train_ds = SST2TorchDataset(tokenized[\"train\"], is_test=False)\n","    dev_ds   = SST2TorchDataset(tokenized[\"validation\"], is_test=False)\n","    BATCH_SIZE = 64\n","    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n","    dev_dl   = DataLoader(dev_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n","\n","    models = {\n","        \"BasicRNN\": BasicRNNSentiment(vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0.3, pad_idx=pad_id, num_classes=2),\n","        \"GRU\":      GRUSentiment(   vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0.3, pad_idx=pad_id, num_classes=2),\n","        \"LSTM\":     LSTMSentiment(  vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0.3, pad_idx=pad_id, num_classes=2),\n","        \"BiLSTM\":   BiLSTMSentiment(vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, bidirectional=True, dropout=0.3, pad_idx=pad_id, num_classes=2),\n","    }\n","\n","    histories = {}\n","    summaries = {}\n","    checkpoints = {}\n","    os.makedirs(\"figs\", exist_ok=True)\n","    os.makedirs(\"outputs\", exist_ok=True)\n","\n","    for name, mdl in models.items():\n","        hist, summ, ckpt = train_and_eval_one_model(\n","            model=mdl,\n","            name=name,\n","            train_dl=train_dl,\n","            dev_dl=dev_dl,\n","            device=DEVICE,\n","            epochs=20,\n","            lr=1e-3,\n","            grad_clip=1.0\n","        )\n","        histories[name] = hist\n","        summaries[name] = summ\n","        checkpoints[name] = ckpt\n","\n","        # per-model confusion matrix fig (Figure 3a–d)\n","        import numpy as np\n","        plot_confusion_matrix(np.array(summ[\"cm\"]), name)\n","\n","    # Figure 1 & 2 (all models together)\n","    plot_history(histories)\n","\n","    # Figure 4 (F1 summary bar)\n","    plot_f1_summary(summaries)\n","\n","    # Save CSV summary + histories\n","    csv_path = os.path.join(\"outputs\", \"model_summary.csv\")\n","    with open(csv_path, \"w\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"Model\", \"Dev Acc\", \"Precision\", \"Recall\", \"F1\", \"Dev Loss\", \"ConfusionMatrix\"])\n","        for name, s in summaries.items():\n","            writer.writerow([name, f\"{s['dev_acc']:.4f}\", f\"{s['precision']:.4f}\",\n","                             f\"{s['recall']:.4f}\", f\"{s['f1']:.4f}\", f\"{s['dev_loss']:.4f}\",\n","                             json.dumps(s['cm'])])\n","\n","    with open(os.path.join(\"outputs\", \"histories.json\"), \"w\") as f:\n","        json.dump(histories, f)\n","\n","    print(\"\\n=== Best Dev Metrics ===\")\n","    for name, s in summaries.items():\n","        print(f\"{name:8s} | Acc={s['dev_acc']:.4f}  P={s['precision']:.4f}  R={s['recall']:.4f}  F1={s['f1']:.4f}  | CM={s['cm']}\")\n","    print(f\"Summary CSV: {csv_path}\")\n","    print(\"Figures saved in ./figs\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ca91902140584e6eb33eba7e50baf127","d8a112cb077c4833bdc12102d8ec57cc","bde926e56b3448b9a0ab602536d1f8b2","48dc3c34ceb145afb3f0d23e7cb84e3d","76916aa510964d0c914e13d15dea2656","8b1844d91d294695af1ff6c851cc29a4","9119ef68472549008a3092a7f6f0d156","e71d2c0077fd493d8f904a64c19b4308","b807ce3402434ab096fe0f9add88ced9","e99aed8732d5458e95760e71fee6dad5","7c643a76fbc442bd8dc17fb9cb50383e"]},"id":"GQQpjkdLezvX","executionInfo":{"status":"ok","timestamp":1761165684583,"user_tz":360,"elapsed":2804448,"user":{"displayName":"Wai Phu Paing","userId":"08636835975691737302"}},"outputId":"e7867a5d-06e4-4556-d678-a99299398b23"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/872 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca91902140584e6eb33eba7e50baf127"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[BasicRNN] [Epoch 1] Train loss=0.5268 acc=0.7301 | Dev loss=0.5268 acc=0.7557 f1=0.7609\n","[BasicRNN] [Epoch 2] Train loss=0.3154 acc=0.8735 | Dev loss=0.5500 acc=0.7557 f1=0.7599\n","[BasicRNN] [Epoch 3] Train loss=0.2364 acc=0.9117 | Dev loss=0.5468 acc=0.7913 f1=0.7869\n","[BasicRNN] [Epoch 4] Train loss=0.1932 acc=0.9313 | Dev loss=0.6281 acc=0.7959 f1=0.8053\n","[BasicRNN] [Epoch 5] Train loss=0.1642 acc=0.9420 | Dev loss=0.6730 acc=0.7890 f1=0.7946\n","[BasicRNN] [Epoch 6] Train loss=0.1422 acc=0.9502 | Dev loss=0.6745 acc=0.7856 f1=0.7925\n","[BasicRNN] [Epoch 7] Train loss=0.1255 acc=0.9561 | Dev loss=0.6988 acc=0.7878 f1=0.8013\n","[BasicRNN] [Epoch 8] Train loss=0.1072 acc=0.9633 | Dev loss=0.7291 acc=0.7993 f1=0.8049\n","[BasicRNN] [Epoch 9] Train loss=0.0946 acc=0.9671 | Dev loss=0.7959 acc=0.7970 f1=0.8053\n","[BasicRNN] [Epoch 10] Train loss=0.0840 acc=0.9707 | Dev loss=0.8554 acc=0.8028 f1=0.8041\n","[BasicRNN] [Epoch 11] Train loss=0.0758 acc=0.9739 | Dev loss=0.9434 acc=0.8028 f1=0.8063\n","[BasicRNN] [Epoch 12] Train loss=0.0677 acc=0.9772 | Dev loss=0.9483 acc=0.8073 f1=0.8129\n","[BasicRNN] [Epoch 13] Train loss=0.0617 acc=0.9784 | Dev loss=0.9142 acc=0.7833 f1=0.7921\n","[BasicRNN] [Epoch 14] Train loss=0.0542 acc=0.9818 | Dev loss=0.9980 acc=0.7844 f1=0.7974\n","[BasicRNN] [Epoch 15] Train loss=0.0493 acc=0.9833 | Dev loss=1.0960 acc=0.7970 f1=0.8040\n","[BasicRNN] [Epoch 16] Train loss=0.0463 acc=0.9846 | Dev loss=1.0781 acc=0.7798 f1=0.7918\n","[BasicRNN] [Epoch 17] Train loss=0.0445 acc=0.9850 | Dev loss=1.0873 acc=0.7936 f1=0.8022\n","[BasicRNN] [Epoch 18] Train loss=0.0417 acc=0.9858 | Dev loss=1.1047 acc=0.7821 f1=0.7926\n","[BasicRNN] [Epoch 19] Train loss=0.0392 acc=0.9862 | Dev loss=1.0560 acc=0.7787 f1=0.7881\n","[BasicRNN] [Epoch 20] Train loss=0.0348 acc=0.9884 | Dev loss=1.1349 acc=0.7844 f1=0.7943\n","[GRU] [Epoch 1] Train loss=0.4665 acc=0.7681 | Dev loss=0.5172 acc=0.8005 f1=0.8161\n","[GRU] [Epoch 2] Train loss=0.2474 acc=0.9013 | Dev loss=0.4687 acc=0.8314 f1=0.8346\n","[GRU] [Epoch 3] Train loss=0.1717 acc=0.9351 | Dev loss=0.4913 acc=0.8303 f1=0.8352\n","[GRU] [Epoch 4] Train loss=0.1302 acc=0.9517 | Dev loss=0.5043 acc=0.8360 f1=0.8409\n","[GRU] [Epoch 5] Train loss=0.0999 acc=0.9632 | Dev loss=0.5398 acc=0.8280 f1=0.8352\n","[GRU] [Epoch 6] Train loss=0.0781 acc=0.9718 | Dev loss=0.5716 acc=0.8291 f1=0.8343\n","[GRU] [Epoch 7] Train loss=0.0618 acc=0.9774 | Dev loss=0.6340 acc=0.8154 f1=0.8201\n","[GRU] [Epoch 8] Train loss=0.0510 acc=0.9813 | Dev loss=0.6370 acc=0.8245 f1=0.8324\n","[GRU] [Epoch 9] Train loss=0.0388 acc=0.9858 | Dev loss=0.6989 acc=0.8234 f1=0.8326\n","[GRU] [Epoch 10] Train loss=0.0343 acc=0.9873 | Dev loss=0.7649 acc=0.8142 f1=0.8224\n","[GRU] [Epoch 11] Train loss=0.0279 acc=0.9896 | Dev loss=0.9077 acc=0.8291 f1=0.8379\n","[GRU] [Epoch 12] Train loss=0.0246 acc=0.9908 | Dev loss=0.8488 acc=0.8303 f1=0.8405\n","[GRU] [Epoch 13] Train loss=0.0221 acc=0.9919 | Dev loss=0.8106 acc=0.8050 f1=0.8064\n","[GRU] [Epoch 14] Train loss=0.0184 acc=0.9931 | Dev loss=0.9585 acc=0.8177 f1=0.8215\n","[GRU] [Epoch 15] Train loss=0.0152 acc=0.9943 | Dev loss=1.0552 acc=0.8211 f1=0.8297\n","[GRU] [Epoch 16] Train loss=0.0156 acc=0.9948 | Dev loss=1.0159 acc=0.8222 f1=0.8321\n","[GRU] [Epoch 17] Train loss=0.0154 acc=0.9944 | Dev loss=0.9589 acc=0.8016 f1=0.8118\n","[GRU] [Epoch 18] Train loss=0.0134 acc=0.9955 | Dev loss=0.9631 acc=0.8062 f1=0.8069\n","[GRU] [Epoch 19] Train loss=0.0128 acc=0.9956 | Dev loss=1.0062 acc=0.8050 f1=0.8156\n","[GRU] [Epoch 20] Train loss=0.0109 acc=0.9963 | Dev loss=1.0349 acc=0.8062 f1=0.8181\n","[LSTM] [Epoch 1] Train loss=0.4580 acc=0.7754 | Dev loss=0.5102 acc=0.7867 f1=0.8075\n","[LSTM] [Epoch 2] Train loss=0.2454 acc=0.9017 | Dev loss=0.5138 acc=0.8349 f1=0.8396\n","[LSTM] [Epoch 3] Train loss=0.1721 acc=0.9340 | Dev loss=0.5508 acc=0.8142 f1=0.8254\n","[LSTM] [Epoch 4] Train loss=0.1301 acc=0.9522 | Dev loss=0.5358 acc=0.8268 f1=0.8313\n","[LSTM] [Epoch 5] Train loss=0.1024 acc=0.9629 | Dev loss=0.6023 acc=0.8245 f1=0.8306\n","[LSTM] [Epoch 6] Train loss=0.0814 acc=0.9707 | Dev loss=0.6312 acc=0.8280 f1=0.8315\n","[LSTM] [Epoch 7] Train loss=0.0649 acc=0.9768 | Dev loss=0.6373 acc=0.8222 f1=0.8276\n","[LSTM] [Epoch 8] Train loss=0.0519 acc=0.9806 | Dev loss=0.6718 acc=0.8257 f1=0.8296\n","[LSTM] [Epoch 9] Train loss=0.0416 acc=0.9848 | Dev loss=0.6597 acc=0.8268 f1=0.8290\n","[LSTM] [Epoch 10] Train loss=0.0342 acc=0.9873 | Dev loss=0.7728 acc=0.8280 f1=0.8355\n","[LSTM] [Epoch 11] Train loss=0.0298 acc=0.9884 | Dev loss=0.8597 acc=0.8200 f1=0.8303\n","[LSTM] [Epoch 12] Train loss=0.0253 acc=0.9903 | Dev loss=0.9833 acc=0.8280 f1=0.8337\n","[LSTM] [Epoch 13] Train loss=0.0214 acc=0.9920 | Dev loss=0.8430 acc=0.8337 f1=0.8263\n","[LSTM] [Epoch 14] Train loss=0.0218 acc=0.9916 | Dev loss=0.8405 acc=0.8050 f1=0.8156\n","[LSTM] [Epoch 15] Train loss=0.0195 acc=0.9928 | Dev loss=0.8048 acc=0.8096 f1=0.8180\n","[LSTM] [Epoch 16] Train loss=0.0163 acc=0.9935 | Dev loss=0.8478 acc=0.8154 f1=0.8156\n","[LSTM] [Epoch 17] Train loss=0.0162 acc=0.9939 | Dev loss=0.9305 acc=0.8131 f1=0.8133\n","[LSTM] [Epoch 18] Train loss=0.0131 acc=0.9948 | Dev loss=0.9865 acc=0.8222 f1=0.8291\n","[LSTM] [Epoch 19] Train loss=0.0140 acc=0.9949 | Dev loss=0.9064 acc=0.8073 f1=0.8197\n","[LSTM] [Epoch 20] Train loss=0.0125 acc=0.9953 | Dev loss=1.1669 acc=0.7936 f1=0.8093\n","[BiLSTM] [Epoch 1] Train loss=0.4432 acc=0.7849 | Dev loss=0.4139 acc=0.8062 f1=0.8077\n","[BiLSTM] [Epoch 2] Train loss=0.2325 acc=0.9077 | Dev loss=0.5049 acc=0.8211 f1=0.8093\n","[BiLSTM] [Epoch 3] Train loss=0.1557 acc=0.9412 | Dev loss=0.4794 acc=0.8211 f1=0.8274\n","[BiLSTM] [Epoch 4] Train loss=0.1139 acc=0.9582 | Dev loss=0.5573 acc=0.8028 f1=0.8080\n","[BiLSTM] [Epoch 5] Train loss=0.0842 acc=0.9696 | Dev loss=0.6394 acc=0.7982 f1=0.8128\n","[BiLSTM] [Epoch 6] Train loss=0.0641 acc=0.9770 | Dev loss=0.8165 acc=0.7959 f1=0.8110\n","[BiLSTM] [Epoch 7] Train loss=0.0497 acc=0.9827 | Dev loss=0.6732 acc=0.8211 f1=0.8278\n","[BiLSTM] [Epoch 8] Train loss=0.0372 acc=0.9874 | Dev loss=0.8011 acc=0.8108 f1=0.8148\n","[BiLSTM] [Epoch 9] Train loss=0.0300 acc=0.9890 | Dev loss=0.9075 acc=0.8211 f1=0.8267\n","[BiLSTM] [Epoch 10] Train loss=0.0247 acc=0.9910 | Dev loss=0.9954 acc=0.8096 f1=0.8143\n","[BiLSTM] [Epoch 11] Train loss=0.0189 acc=0.9938 | Dev loss=1.0322 acc=0.8200 f1=0.8250\n","[BiLSTM] [Epoch 12] Train loss=0.0183 acc=0.9938 | Dev loss=1.0554 acc=0.8188 f1=0.8275\n","[BiLSTM] [Epoch 13] Train loss=0.0147 acc=0.9951 | Dev loss=0.9542 acc=0.8349 f1=0.8364\n","[BiLSTM] [Epoch 14] Train loss=0.0131 acc=0.9957 | Dev loss=1.1834 acc=0.8062 f1=0.8169\n","[BiLSTM] [Epoch 15] Train loss=0.0121 acc=0.9961 | Dev loss=1.1004 acc=0.8245 f1=0.8298\n","[BiLSTM] [Epoch 16] Train loss=0.0103 acc=0.9968 | Dev loss=1.3258 acc=0.8165 f1=0.8257\n","[BiLSTM] [Epoch 17] Train loss=0.0097 acc=0.9970 | Dev loss=1.2766 acc=0.8234 f1=0.8270\n","[BiLSTM] [Epoch 18] Train loss=0.0102 acc=0.9969 | Dev loss=1.2014 acc=0.8165 f1=0.8182\n","[BiLSTM] [Epoch 19] Train loss=0.0088 acc=0.9973 | Dev loss=1.1486 acc=0.8222 f1=0.8237\n","[BiLSTM] [Epoch 20] Train loss=0.0052 acc=0.9982 | Dev loss=1.3692 acc=0.8108 f1=0.8101\n","\n","=== Best Dev Metrics ===\n","BasicRNN | Acc=0.8073  P=0.8040  R=0.8221  F1=0.8129  | CM=[[339, 89], [79, 365]]\n","GRU      | Acc=0.8360  P=0.8308  R=0.8514  F1=0.8409  | CM=[[351, 77], [66, 378]]\n","LSTM     | Acc=0.8349  P=0.8304  R=0.8491  F1=0.8396  | CM=[[351, 77], [67, 377]]\n","BiLSTM   | Acc=0.8349  P=0.8440  R=0.8288  F1=0.8364  | CM=[[360, 68], [76, 368]]\n","Summary CSV: outputs/model_summary.csv\n","Figures saved in ./figs\n"]}]}]}